{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Introduction Brief description This platform is a collection of tools that aim to automate the process of collecting memory samples of numerous micro-controllers and their subsequent storage for future analysis. The memory data gathered will then be used mainly for SRAM-based Physical Unclonable Functions (PUF) analysis, as it is normally very difficult to gather enough data for this type of analysis. Motivation The purpose of our open platform is twofold: To provide easy access to a comprehensive database that includes thousands of samples of multiple boards, and to share the platform to carry out tailored experiments related to NBTI and data remanence effects. Our open platform succeeds in both aspects. From the economic point of view, our platform will save many resources to the users in terms of money (buying hundreds of devices) and time (collecting thousands of samples). Using the available raw-data, any user of the platform can carry out their own experiments (e.g. design of new post-processing, find systematic variations, etc.) with an enough number of samples and devices to consider the experiment statistically significant. Besides, the extra information provided (operating conditions, wafer position, etc) will open new possibilities to find vulnerabilities and develop new metrics. As a totally new feature, to the best of our knowledge unique up to the date, we offer the user the possibility of interaction with the boards by controlling the switch On/Off time of the micro-controllers (data remanence studies) and writing custom values in the SRAM (NBTI studies). Data Availability An Station has been deployed at TIMA Laboratory in Grenoble, France. The data gathered in this station is publicly available online through the following website","title":"Introduction"},{"location":"#introduction","text":"","title":"Introduction"},{"location":"#brief-description","text":"This platform is a collection of tools that aim to automate the process of collecting memory samples of numerous micro-controllers and their subsequent storage for future analysis. The memory data gathered will then be used mainly for SRAM-based Physical Unclonable Functions (PUF) analysis, as it is normally very difficult to gather enough data for this type of analysis.","title":"Brief description"},{"location":"#motivation","text":"The purpose of our open platform is twofold: To provide easy access to a comprehensive database that includes thousands of samples of multiple boards, and to share the platform to carry out tailored experiments related to NBTI and data remanence effects. Our open platform succeeds in both aspects. From the economic point of view, our platform will save many resources to the users in terms of money (buying hundreds of devices) and time (collecting thousands of samples). Using the available raw-data, any user of the platform can carry out their own experiments (e.g. design of new post-processing, find systematic variations, etc.) with an enough number of samples and devices to consider the experiment statistically significant. Besides, the extra information provided (operating conditions, wafer position, etc) will open new possibilities to find vulnerabilities and develop new metrics. As a totally new feature, to the best of our knowledge unique up to the date, we offer the user the possibility of interaction with the boards by controlling the switch On/Off time of the micro-controllers (data remanence studies) and writing custom values in the SRAM (NBTI studies).","title":"Motivation"},{"location":"#data-availability","text":"An Station has been deployed at TIMA Laboratory in Grenoble, France. The data gathered in this station is publicly available online through the following website","title":"Data Availability"},{"location":"commands/","text":"Executing commands A command refers to an operation that the station can carry out. All of the information necesary to carry such operation should be written to the header of the packet. Command Command code Description ACK 1 Packet acknowledge PING 2 Discover devices in a chain READ 3 Read a region of memory from a device WRITE 4 Write values to a region of memory of a device EXEC 5 Execut custom loaded code from memory ERR 255 Error during transmision Info The script send_command.py can be used to send one of the predefined commands above to a running agent from the CLI. Once an agent has been deployed, and a reader has been assigned to it, new messages can be sent to the agent directly as seen below. Sending a message with an agent from fenneq import Agent # URL and exchange_name have to be the same as the other agents url = \"amqp://user:pass@localhost\" exchange_name = \"sram_commands\" topic = \"sram.discovery\" agent = Agent ( url , exchange_name , topic , Agent . JSON ) msg = { 'method' : 'READ' } # Message will be sent to sram.discovery agent . send ( msg ) # The message can be sent to another topic with the same agent agent . send ( msg , name = \"sram.nucleo\" ) Station commands The responses from the station are logged into rabbitmq as a JSON with the fields status , msg Status This command checks the current status of the station at any given time. It returns the { 'status' : 'OK' , 'msg' : { 'state' : 'ON' , 'devices' : []}} Ping command { 'status' : 'OK' , 'msg' : { 'state' : 'ON' , 'devices' : []}} Read command { 'status' : 'OK' , 'msg' : { 'state' : 'ON' , 'devices' : []}} Write command { 'status' : 'OK' , 'msg' : { 'state' : 'ON' , 'devices' : []}} Write invert command { 'status' : 'OK' , 'msg' : { 'state' : 'ON' , 'devices' : []}}","title":"Commands"},{"location":"commands/#executing-commands","text":"A command refers to an operation that the station can carry out. All of the information necesary to carry such operation should be written to the header of the packet. Command Command code Description ACK 1 Packet acknowledge PING 2 Discover devices in a chain READ 3 Read a region of memory from a device WRITE 4 Write values to a region of memory of a device EXEC 5 Execut custom loaded code from memory ERR 255 Error during transmision Info The script send_command.py can be used to send one of the predefined commands above to a running agent from the CLI. Once an agent has been deployed, and a reader has been assigned to it, new messages can be sent to the agent directly as seen below. Sending a message with an agent from fenneq import Agent # URL and exchange_name have to be the same as the other agents url = \"amqp://user:pass@localhost\" exchange_name = \"sram_commands\" topic = \"sram.discovery\" agent = Agent ( url , exchange_name , topic , Agent . JSON ) msg = { 'method' : 'READ' } # Message will be sent to sram.discovery agent . send ( msg ) # The message can be sent to another topic with the same agent agent . send ( msg , name = \"sram.nucleo\" )","title":"Executing commands"},{"location":"commands/#station-commands","text":"The responses from the station are logged into rabbitmq as a JSON with the fields status , msg","title":"Station commands"},{"location":"commands/#status","text":"This command checks the current status of the station at any given time. It returns the { 'status' : 'OK' , 'msg' : { 'state' : 'ON' , 'devices' : []}}","title":"Status"},{"location":"commands/#ping-command","text":"{ 'status' : 'OK' , 'msg' : { 'state' : 'ON' , 'devices' : []}}","title":"Ping command"},{"location":"commands/#read-command","text":"{ 'status' : 'OK' , 'msg' : { 'state' : 'ON' , 'devices' : []}}","title":"Read command"},{"location":"commands/#write-command","text":"{ 'status' : 'OK' , 'msg' : { 'state' : 'ON' , 'devices' : []}}","title":"Write command"},{"location":"commands/#write-invert-command","text":"{ 'status' : 'OK' , 'msg' : { 'state' : 'ON' , 'devices' : []}}","title":"Write invert command"},{"location":"communication/","text":"Communication protocol As it has been previously stated the reader is in charge of creating the necesary packets that will be sent to the devices to carry out the commands. The commands are sent to the station by means of a RabbitMQ channel, and the reader is dispatched through a fenneq agent listening for the proper messages. The documentation of fenneq provides in detail explanations on how to As an example, a generic reader can be implemented in python with the following interface: Definition of the Reader Interface class Reader : def __init__ ( self , name ): self . devices = [] self . name = name def send ( self , data : bytes ): raise NotImplementedError def receive ( self ): raise NotImplementedError () Info This station already includes the definition of a reader for STM32 devices by means of USART. Example of combining an agent and a reader from fenneq import Agent from reader import STM32Reader from database import LogLevel # Reader called DISCOVERY, listening on USB port 1 @ 125_000 reader = STM32Reader ( \"DISCOVERY\" , 1 , baudrate = 125_000 ) # Url to connect to the RabbitMQ interchange url = \"amqp://user:pass@localhost\" exchange_name = \"sram_commands\" # Commands sent to sram.discovery will be read by this agent. # For more detail on how to setup topics follow the official documentation # rabbitmq topic documentation topic = \"sram.discovery\" agent = Agent ( url , exchange_name , topic , Agent . JSON ) # The function handle_read will be called when a message from sram_commands # sent to sram.discovery contains the values {'method': \"READ\"} @agent . on ({ \"method\" : \"READ\" }) def handle_read ( ch , method_frame , props , body ): res = reader . handle_read ( body ) # Multiple functions can be assigned to the same message. @agent . on ({ \"method\" : \"READ\" }) def handle_read_second ( ch , method_frame , props , body ): res = reader . handle_read_second ( body ) # Moreover, multiple agents can use the same callback. @agent . on ({ \"method\" : \"READ\" }) @agent_two . on ({ \"command\" : \"read\" }) def handle_read ( ch , method_frame , props , body ): res = reader . handle_read_second ( body ) Packet based protocol The communication between a reader and a device chain is performed using a custom packet based protocol. The source code documentation of the packet can be found :doc: here <packet> . Field Encoding Description Method uint8_t Type of packet. See commands PIC uint16_t Position In Chain . Index of the device in the chain Options uint16_t Metadata for the packet UID char[25] Universal ID of the device Checksum uint32_t Checksum of the packet Data uint8_t[DATA_SIZE] Actual data of the packet The DATA_SIZE can be defined by the user, but it has to be small than the smallest SRAM size of a device in the chain. Working with packets Example of creation of a packet packet = Packet () # Generate a packet with default values # The following are the default values packet . with_method ( Method . Ping ) packet . with_options ( 0x0 ) packet . with_checksum ( 0 ) packet . with_pic ( 1 ) packet . with_uid ( \"DEVICE ID\" ) packet . with_data ([ 0x0 , ... , 0x0 ]) packet . craft () # Craft the packet to send it # The packet can now be used by calling `to_bytes` print ( packet . to_bytes ()) Warning Even if the packet has the default configuration, is is necesary to craft it before sending it, otherwise it will raise a ValueError . The method is_crafted returns True if the packet is ready to be sent.","title":"Communication"},{"location":"communication/#communication-protocol","text":"As it has been previously stated the reader is in charge of creating the necesary packets that will be sent to the devices to carry out the commands. The commands are sent to the station by means of a RabbitMQ channel, and the reader is dispatched through a fenneq agent listening for the proper messages. The documentation of fenneq provides in detail explanations on how to As an example, a generic reader can be implemented in python with the following interface: Definition of the Reader Interface class Reader : def __init__ ( self , name ): self . devices = [] self . name = name def send ( self , data : bytes ): raise NotImplementedError def receive ( self ): raise NotImplementedError () Info This station already includes the definition of a reader for STM32 devices by means of USART. Example of combining an agent and a reader from fenneq import Agent from reader import STM32Reader from database import LogLevel # Reader called DISCOVERY, listening on USB port 1 @ 125_000 reader = STM32Reader ( \"DISCOVERY\" , 1 , baudrate = 125_000 ) # Url to connect to the RabbitMQ interchange url = \"amqp://user:pass@localhost\" exchange_name = \"sram_commands\" # Commands sent to sram.discovery will be read by this agent. # For more detail on how to setup topics follow the official documentation # rabbitmq topic documentation topic = \"sram.discovery\" agent = Agent ( url , exchange_name , topic , Agent . JSON ) # The function handle_read will be called when a message from sram_commands # sent to sram.discovery contains the values {'method': \"READ\"} @agent . on ({ \"method\" : \"READ\" }) def handle_read ( ch , method_frame , props , body ): res = reader . handle_read ( body ) # Multiple functions can be assigned to the same message. @agent . on ({ \"method\" : \"READ\" }) def handle_read_second ( ch , method_frame , props , body ): res = reader . handle_read_second ( body ) # Moreover, multiple agents can use the same callback. @agent . on ({ \"method\" : \"READ\" }) @agent_two . on ({ \"command\" : \"read\" }) def handle_read ( ch , method_frame , props , body ): res = reader . handle_read_second ( body )","title":"Communication protocol"},{"location":"communication/#packet-based-protocol","text":"The communication between a reader and a device chain is performed using a custom packet based protocol. The source code documentation of the packet can be found :doc: here <packet> . Field Encoding Description Method uint8_t Type of packet. See commands PIC uint16_t Position In Chain . Index of the device in the chain Options uint16_t Metadata for the packet UID char[25] Universal ID of the device Checksum uint32_t Checksum of the packet Data uint8_t[DATA_SIZE] Actual data of the packet The DATA_SIZE can be defined by the user, but it has to be small than the smallest SRAM size of a device in the chain.","title":"Packet based protocol"},{"location":"communication/#working-with-packets","text":"Example of creation of a packet packet = Packet () # Generate a packet with default values # The following are the default values packet . with_method ( Method . Ping ) packet . with_options ( 0x0 ) packet . with_checksum ( 0 ) packet . with_pic ( 1 ) packet . with_uid ( \"DEVICE ID\" ) packet . with_data ([ 0x0 , ... , 0x0 ]) packet . craft () # Craft the packet to send it # The packet can now be used by calling `to_bytes` print ( packet . to_bytes ()) Warning Even if the packet has the default configuration, is is necesary to craft it before sending it, otherwise it will raise a ValueError . The method is_crafted returns True if the packet is ready to be sent.","title":"Working with packets"},{"location":"components/","text":"Platform components This platform is composed of multiple components working together. Message Broker RabbitMQ is the The message broker chosen Database The main purpose of the database is to store the SRAM from the devices connected to the station as well as Note The database chosen for this project is PostreSQL . However another database can be used instead by configuring the DBManager. Dispatcher A dispatcher is one of the main components of the platform. A dispatcher will wait for messages until a match is found. A dispatcher should have only one reader atached, although mulitple readers can react to the same messages. Reader The purpose of the reader is to communicate the station with the devices. To guarantee that different devices can be added to the station the reader (e.g. a reader for STM32 devices). The reader is the component that manages the communication between the devices and the station itself. Device A device is the smallest unit of the station. These are the physical devices whose memory will be read and studied. This framework is designed to work with microcontrollers but it should work with other devices as long as the communication protocol is implemented. Info In this documentation, the terms device and board are equivalent.","title":"Components"},{"location":"components/#platform-components","text":"This platform is composed of multiple components working together.","title":"Platform components"},{"location":"components/#message-broker","text":"RabbitMQ is the The message broker chosen","title":"Message Broker"},{"location":"components/#database","text":"The main purpose of the database is to store the SRAM from the devices connected to the station as well as Note The database chosen for this project is PostreSQL . However another database can be used instead by configuring the DBManager.","title":"Database"},{"location":"components/#dispatcher","text":"A dispatcher is one of the main components of the platform. A dispatcher will wait for messages until a match is found. A dispatcher should have only one reader atached, although mulitple readers can react to the same messages.","title":"Dispatcher"},{"location":"components/#reader","text":"The purpose of the reader is to communicate the station with the devices. To guarantee that different devices can be added to the station the reader (e.g. a reader for STM32 devices). The reader is the component that manages the communication between the devices and the station itself.","title":"Reader"},{"location":"components/#device","text":"A device is the smallest unit of the station. These are the physical devices whose memory will be read and studied. This framework is designed to work with microcontrollers but it should work with other devices as long as the communication protocol is implemented. Info In this documentation, the terms device and board are equivalent.","title":"Device"},{"location":"database/","text":"Storage The information is stored in a PostreSQL database. In the python code, the database is managed with the sqlalchemy ORM. That means that the tables in the database can be created from a python class. All of the logic concerning the database is carried out by the DBManager . Storing samples and sensor information SRAM samples and sensors are stored as the python classes Sample and Sensor , respectively. Example of storing a sample and sensor url = \"postgres://username:password@localhost:5432/database\" db = DBManager ( url ) sample = Sample ( uid = \"DEVICE ID\" , board_id = \"NUCLEO\" , pic = 1 , address = \"0x20000000\" , data = \",\" . join ([ str ( d ) for d in range ( 1024 )]), created_at = datetime . now (), ) db . insert ( sample ) sensor = Sensor ( uid = \"DEVICE ID\" , board_id = \"NUCLEO\" , temperature = 27 , voltage = 3300 , ) db . insert ( sensor ) db . commit () Storing the log of operations A sample is stored as a Log. The different levels for the messages are \"INFO\", \"CRITICAL\" and \"WARNING\" as described in LogLevel. Example of storing a log name = \"agent_name\" level = LogLevel . INFO message = \"This is a simple message\" db . log ( self , name , level , message )","title":"Storage"},{"location":"database/#storage","text":"The information is stored in a PostreSQL database. In the python code, the database is managed with the sqlalchemy ORM. That means that the tables in the database can be created from a python class. All of the logic concerning the database is carried out by the DBManager .","title":"Storage"},{"location":"database/#storing-samples-and-sensor-information","text":"SRAM samples and sensors are stored as the python classes Sample and Sensor , respectively. Example of storing a sample and sensor url = \"postgres://username:password@localhost:5432/database\" db = DBManager ( url ) sample = Sample ( uid = \"DEVICE ID\" , board_id = \"NUCLEO\" , pic = 1 , address = \"0x20000000\" , data = \",\" . join ([ str ( d ) for d in range ( 1024 )]), created_at = datetime . now (), ) db . insert ( sample ) sensor = Sensor ( uid = \"DEVICE ID\" , board_id = \"NUCLEO\" , temperature = 27 , voltage = 3300 , ) db . insert ( sensor ) db . commit ()","title":"Storing samples and sensor information"},{"location":"database/#storing-the-log-of-operations","text":"A sample is stored as a Log. The different levels for the messages are \"INFO\", \"CRITICAL\" and \"WARNING\" as described in LogLevel. Example of storing a log name = \"agent_name\" level = LogLevel . INFO message = \"This is a simple message\" db . log ( self , name , level , message )","title":"Storing the log of operations"},{"location":"devices/","text":"Devices This platform was created with the intention of collecting data from micro-controllers. The devices we mainly have access to are the STM32L152RE and the STM32L152RCT6 . One of the limitations we have to solve is to maximize the number of devices we can connect to a computer. The USB protocol has a limitations of 127 devices in total, including USB hubs. Moreover the process of connecting or removing devices from a station will be very time consuming. In order to have relevant statistically relevant analysis for SRAM-based PUF, we need a very large number of devices (hundreds at least) so we need to use another solution for this. To solve this problem, the devices are connected in scan chains , that is, the computer connects to a device (the start of the chain) and the next device is connected to the device before it. Doing this the only limitation is having a power supply strong enough to keep all the connected devices turned on. The devices communicate between them by using the USART, which is very simple to configure and provides speeds of communication fast enough for this application. In order to control the devices that are connected to a chain, the station keeps track of each device unique id and their position in the chain (referred in the code as pic ). STM32 devices contain 96 internal bits stored in the SRAM, that can be used as a unique identifier of the device. In the case of using other devices, the user would need to store a unique id for each device in memory to be able to identify them in the future. To keep track of the position, each packet has a field called pic, that gets incremented every time a packet travels downstream. (Oposite as how the Time To Live of an IPC packet gets decremented after each jump). The process of gathering the information of every devices is through the PING command, described in detail commands . We can see in the following diagram, an example of how the different devices create chains and how different chains can be connected to a computer. USB Rx -> Tx Rx -> Tx Rx -> Tx Computer ------> Device ----------> Device ----------> ... ----------> Device | Tx -> Rx Tx -> Rx Tx -> Rx | | | | USB Rx -> Tx |-----> Device ----------> Device Tx -> Rx The software running on the devices has been created such that all devices run the same code, independently of their position in the chain. This makes the process of adding and removing boards very simple. Besides that, devices of different devices can be connected in the same scan chain, as each device is aware of its own SRAM size and can react to packets that ask for commands that cannot be carried out for an specific amount of memory. Since the Regarding the communication direction, there are two distinct directions: Upstream From the device to the station. Downstream From a device to the next device. This distinction will be important specially in the device source code. Each device allocates a buffer for each direction as to be able to receive data from both directions at the same time. This should not happen in real life, but one of the priority of the station is to guarantee the integrity of the data sent or received from a device. For that, only one of the buffers should be used at real time. This separation also makes the code very simple as it is very simple to setup the interreptions for the communication protocol to store the data received directly in the buffer. Info As it was said before, this platform was created with the intention of gathering data from micro-controllers. Other types of devices can be connected to the station as long as they use the same packet based protocol. If the user wants to use another communication protocol, they will need to write their own Device Reader and register the new reader to an agent.","title":"Devices"},{"location":"devices/#devices","text":"This platform was created with the intention of collecting data from micro-controllers. The devices we mainly have access to are the STM32L152RE and the STM32L152RCT6 . One of the limitations we have to solve is to maximize the number of devices we can connect to a computer. The USB protocol has a limitations of 127 devices in total, including USB hubs. Moreover the process of connecting or removing devices from a station will be very time consuming. In order to have relevant statistically relevant analysis for SRAM-based PUF, we need a very large number of devices (hundreds at least) so we need to use another solution for this. To solve this problem, the devices are connected in scan chains , that is, the computer connects to a device (the start of the chain) and the next device is connected to the device before it. Doing this the only limitation is having a power supply strong enough to keep all the connected devices turned on. The devices communicate between them by using the USART, which is very simple to configure and provides speeds of communication fast enough for this application. In order to control the devices that are connected to a chain, the station keeps track of each device unique id and their position in the chain (referred in the code as pic ). STM32 devices contain 96 internal bits stored in the SRAM, that can be used as a unique identifier of the device. In the case of using other devices, the user would need to store a unique id for each device in memory to be able to identify them in the future. To keep track of the position, each packet has a field called pic, that gets incremented every time a packet travels downstream. (Oposite as how the Time To Live of an IPC packet gets decremented after each jump). The process of gathering the information of every devices is through the PING command, described in detail commands . We can see in the following diagram, an example of how the different devices create chains and how different chains can be connected to a computer. USB Rx -> Tx Rx -> Tx Rx -> Tx Computer ------> Device ----------> Device ----------> ... ----------> Device | Tx -> Rx Tx -> Rx Tx -> Rx | | | | USB Rx -> Tx |-----> Device ----------> Device Tx -> Rx The software running on the devices has been created such that all devices run the same code, independently of their position in the chain. This makes the process of adding and removing boards very simple. Besides that, devices of different devices can be connected in the same scan chain, as each device is aware of its own SRAM size and can react to packets that ask for commands that cannot be carried out for an specific amount of memory. Since the Regarding the communication direction, there are two distinct directions: Upstream From the device to the station. Downstream From a device to the next device. This distinction will be important specially in the device source code. Each device allocates a buffer for each direction as to be able to receive data from both directions at the same time. This should not happen in real life, but one of the priority of the station is to guarantee the integrity of the data sent or received from a device. For that, only one of the buffers should be used at real time. This separation also makes the code very simple as it is very simple to setup the interreptions for the communication protocol to store the data received directly in the buffer. Info As it was said before, this platform was created with the intention of gathering data from micro-controllers. Other types of devices can be connected to the station as long as they use the same packet based protocol. If the user wants to use another communication protocol, they will need to write their own Device Reader and register the new reader to an agent.","title":"Devices"},{"location":"logging/","text":"Logging The way commands are added to a dispatcher is through the add_command() method. Implementation example of add_command() def add_command ( self , handler , func , ** options ): \"Add a command to a dispatcher\" @self . agent . on ( handler , ** options ) def handler_fn ( * args , ** kwargs ): response = func ( * args , ** kwargs ) if response : message = { \"handler\" : kwargs [ \"body\" ], \"response\" : response } self . log . send ( msg = message ) The result of the command, if any, will be logged automatically to the logging channel specified in the Dispatcher. Function template whose result is logged. from sramplatform import Status , LogLevel def custom_fn ( * args , ** kwargs ): if True : return { 'status' : Status . OK } else : return { 'status' : Status . OK , 'level' : LogLevel . Info , 'msg' : \"Error message\" } Grafana Dashboard Grafana is a multi-platform open source analytics and interactive visualization web application. It allows to display analytics in realtime. Info Grafana import and export","title":"Logging"},{"location":"logging/#logging","text":"The way commands are added to a dispatcher is through the add_command() method. Implementation example of add_command() def add_command ( self , handler , func , ** options ): \"Add a command to a dispatcher\" @self . agent . on ( handler , ** options ) def handler_fn ( * args , ** kwargs ): response = func ( * args , ** kwargs ) if response : message = { \"handler\" : kwargs [ \"body\" ], \"response\" : response } self . log . send ( msg = message ) The result of the command, if any, will be logged automatically to the logging channel specified in the Dispatcher. Function template whose result is logged. from sramplatform import Status , LogLevel def custom_fn ( * args , ** kwargs ): if True : return { 'status' : Status . OK } else : return { 'status' : Status . OK , 'level' : LogLevel . Info , 'msg' : \"Error message\" }","title":"Logging"},{"location":"logging/#grafana-dashboard","text":"Grafana is a multi-platform open source analytics and interactive visualization web application. It allows to display analytics in realtime. Info Grafana import and export","title":"Grafana Dashboard"},{"location":"setup/","text":"Open-platform for the acquisition of SRAM-based PUFs from Micro-Controllers Setup guide This section will describe the requirements needed to deploy a station. Here we describe first the software requirements followed by the hardware requirements and lastly, their physical installation and deployment. Python dependencies Python dependencies can be installed by using the requirements.txt file provided. It is recomended, but not mandatory, to create a virtual environment to install the requirements. The following code describes the process of creating a virtual env and installing the dependencies. Using pip $ python3 -m venv /path/to/virtual_env $ source /path/to/virtual_env/bin/activate $ pip install -r requirements.txt $ deactivate # To exit the virtual environment Using poetry $ cd /path/to/SRAMPlatform $ poetry install Device source code This project contains code for two different STM32 boards. Each project is managed by STM32Cube. Devices should be programmed using the same software. Other dependencies The platform relies on RabbitMQ <https://www.rabbitmq.com/> for the communication and queing of commands, and PostgreSQL <https://www.postgresql.org/> for the storage. They can be deployed easily thanks to Docker <https://www.docker.com/> _. Info The access to the database is carried out with an ORM, so the user should be able to swap PostreSQL for another database in case they need. The file docker-compose.yml provides the template necesary to launch those services. However, it is necesary to update the configuration values in the file before deploying. The main parameters to modify are user and password of both RabbitMQ and PostgreSQL. The other important parameter is the volume configuration for postgreSQL, (e.g. where to store the data in the computer). The path before the semicolon points to the path in the computer where to store the samples. The path after the semicolorn should not be modified . Note We can think of docker as a virtual machine. We can provide some paths (here volumes ) in the computer that will get linked to a path inside the container. The syntax is path_in_computer:path_in_docker . Example of docker-compose file version : \"3.9\" services : rabbitmq : image : rabbitmq:3-management-alpine container_name : 'rabbitmq' environment : RABBITMQ_ERLANG_COOKIE : \"ERLANG COOKIE\" RABBITMQ_DEFAULT_USER : \"username\" RABBITMQ_DEFAULT_PASS : \"password\" RABBITMQ_DEFAULT_VHOST : \"/\" ports : - 5672:5672 - 15672:15672 networks : - rabbitmq_net postgres : image : postgres:latest container_name : \"postgre\" environment : POSTGRES_USER : \"username\" POSTGRES_PASSWORD : \"password\" POSTGRES_DB : \"database\" ports : - 5432:5432 volumes : - /path/to/db:/var/lib/postgresql/data networks : rabbitmq_net : driver : bridge Start services with docker-compose $ docker-compose up -d -f /path/to/docker-compose.yml $ docker-compose up -d # If in the same path as docker-compose.yml Stop docker services $ docker-compose down # In the same path as docker-compose.yml Deployment services Once all the software is installed and the hardware is properly connected, the station should be ready for deployment. The deployment of the station can be carried out with the use of systemd services. #!/usr/bin/env python3 from sramplatform import Dispatcher , ConnParameters # Custom implementation of Reader from customreader import CustomReader reader = CustomReader ( \"Discovery\" , 0 , 125_000 ) params = ConnParameters ( \"rabbitmq_user\" , \"rabbitmq_pass\" ) platform = Dispatcher ( params , \"exchange_commands\" , \"station_name\" , \"exchange_logs\" ) platform . add_command ({ \"method\" : \"read\" }, reader . handle_read ) platform . add_command ({ \"method\" : \"write\" , \"data\" : True }, reader . handle_write ) if __name__ == '__main__' : platform . run () [Unit] Description=SRAM Reliability Platform After=network.target [Service] Type=simple Restart=always RestartSec=5 WorkingDirectory=/path/to/SRAMPlatform ExecStart=/path/to/virtualenv/bin/python3 main.py [Install] WantedBy=multi-user.target Operations can be scheduled by using the send_command.py script provided and a systemd timer (very similar to a cron job). The following example illustrates how to create the files necesary to power off the platform every friday at 17:00. [Unit] Description=Power off the SRAM Platform [Timer] OnCalendar=Fri *-*-* 17:00:00 Persistent=true [Install] WantedBy=timers.target [Unit] Description=Power off the SRAM Platform After=network.target [Service] Type=oneshot RemainAfterExit=true WorkingDirectory=/path/to/SRAMPlatform ExecStart=/path/to/virtualenv/bin/python3 send_command.py \"OFF\" [Install] WantedBy=multi-user.target","title":"Setup"},{"location":"setup/#setup-guide","text":"This section will describe the requirements needed to deploy a station. Here we describe first the software requirements followed by the hardware requirements and lastly, their physical installation and deployment.","title":"Setup guide"},{"location":"setup/#python-dependencies","text":"Python dependencies can be installed by using the requirements.txt file provided. It is recomended, but not mandatory, to create a virtual environment to install the requirements. The following code describes the process of creating a virtual env and installing the dependencies. Using pip $ python3 -m venv /path/to/virtual_env $ source /path/to/virtual_env/bin/activate $ pip install -r requirements.txt $ deactivate # To exit the virtual environment Using poetry $ cd /path/to/SRAMPlatform $ poetry install","title":"Python dependencies"},{"location":"setup/#device-source-code","text":"This project contains code for two different STM32 boards. Each project is managed by STM32Cube. Devices should be programmed using the same software.","title":"Device source code"},{"location":"setup/#other-dependencies","text":"The platform relies on RabbitMQ <https://www.rabbitmq.com/> for the communication and queing of commands, and PostgreSQL <https://www.postgresql.org/> for the storage. They can be deployed easily thanks to Docker <https://www.docker.com/> _. Info The access to the database is carried out with an ORM, so the user should be able to swap PostreSQL for another database in case they need. The file docker-compose.yml provides the template necesary to launch those services. However, it is necesary to update the configuration values in the file before deploying. The main parameters to modify are user and password of both RabbitMQ and PostgreSQL. The other important parameter is the volume configuration for postgreSQL, (e.g. where to store the data in the computer). The path before the semicolon points to the path in the computer where to store the samples. The path after the semicolorn should not be modified . Note We can think of docker as a virtual machine. We can provide some paths (here volumes ) in the computer that will get linked to a path inside the container. The syntax is path_in_computer:path_in_docker . Example of docker-compose file version : \"3.9\" services : rabbitmq : image : rabbitmq:3-management-alpine container_name : 'rabbitmq' environment : RABBITMQ_ERLANG_COOKIE : \"ERLANG COOKIE\" RABBITMQ_DEFAULT_USER : \"username\" RABBITMQ_DEFAULT_PASS : \"password\" RABBITMQ_DEFAULT_VHOST : \"/\" ports : - 5672:5672 - 15672:15672 networks : - rabbitmq_net postgres : image : postgres:latest container_name : \"postgre\" environment : POSTGRES_USER : \"username\" POSTGRES_PASSWORD : \"password\" POSTGRES_DB : \"database\" ports : - 5432:5432 volumes : - /path/to/db:/var/lib/postgresql/data networks : rabbitmq_net : driver : bridge Start services with docker-compose $ docker-compose up -d -f /path/to/docker-compose.yml $ docker-compose up -d # If in the same path as docker-compose.yml Stop docker services $ docker-compose down # In the same path as docker-compose.yml","title":"Other dependencies"},{"location":"setup/#deployment-services","text":"Once all the software is installed and the hardware is properly connected, the station should be ready for deployment. The deployment of the station can be carried out with the use of systemd services. #!/usr/bin/env python3 from sramplatform import Dispatcher , ConnParameters # Custom implementation of Reader from customreader import CustomReader reader = CustomReader ( \"Discovery\" , 0 , 125_000 ) params = ConnParameters ( \"rabbitmq_user\" , \"rabbitmq_pass\" ) platform = Dispatcher ( params , \"exchange_commands\" , \"station_name\" , \"exchange_logs\" ) platform . add_command ({ \"method\" : \"read\" }, reader . handle_read ) platform . add_command ({ \"method\" : \"write\" , \"data\" : True }, reader . handle_write ) if __name__ == '__main__' : platform . run () [Unit] Description=SRAM Reliability Platform After=network.target [Service] Type=simple Restart=always RestartSec=5 WorkingDirectory=/path/to/SRAMPlatform ExecStart=/path/to/virtualenv/bin/python3 main.py [Install] WantedBy=multi-user.target Operations can be scheduled by using the send_command.py script provided and a systemd timer (very similar to a cron job). The following example illustrates how to create the files necesary to power off the platform every friday at 17:00. [Unit] Description=Power off the SRAM Platform [Timer] OnCalendar=Fri *-*-* 17:00:00 Persistent=true [Install] WantedBy=timers.target [Unit] Description=Power off the SRAM Platform After=network.target [Service] Type=oneshot RemainAfterExit=true WorkingDirectory=/path/to/SRAMPlatform ExecStart=/path/to/virtualenv/bin/python3 send_command.py \"OFF\" [Install] WantedBy=multi-user.target","title":"Deployment services"}]}